<script>
  // UTF-8 parsing rule: 
  // the sum of 1 in first byte stand for how many byte represented a single character
  // each byte behind first byte should begin with 10(like 10xxxxxx)
  // 1 byte represented character:  0xxxxxxx                                                 
  // 2 bytes represented character: 110xxxxx 10xxxxxx
  // 3 bytes represented character: 1110xxxx 10xxxxxx 10xxxxxx
  // position x can be filled with 0 or 1

  function UTF8_Encoding(string) {
    let utf8Arr = [];
    let byteSize = 0;
    for (let index in string) {
      let utf8Character = []
      // get decimal code of Unicode character
      let code = string.codePointAt(index);
      console.log(code)
      //  00000000 to 01111111  in UTF-8 is 
      //   0000000 to 1111111   in binary
      // 0000|0000   0111|1111  transform binary to hexadecimal
      //    0x00   to   0x7f    in hexadecimal
      //     0     to   127     in decimal 
      if (0x00 >= code && code <= 0x7f) { 
        byteSize += 1;
        utf8Character.push(code);

      // fill 110xxxxx 10xxxxxx with 1, get the max of 2 bytes format 11011111 10111111
      // the min of 2 bytes format is decimal 127 + 1 = 128

      // 11000010 10000000 to 11011111 10111111 in UTF-8 is 
      //      00010 000000 to 11111 111111   in binary
      //     0000|1000|0000  0111|1111|1111  transform binary to hexadecimal
      //          0x80     to    0x7ff       in hexadecimal
      //          128      to    2047        in decimal
      } else if (0x80 >= code && code <= 0x7ff) {
        byteSize += 2;
        // transform decimal to 2 bytes format
        utf8Character.push((parseInt(11000000, 2) | (parseInt(11111, 2) & (code >> 6))));
        utf8Character.push((parseInt(10000000, 2) | (parseInt(111111, 2) & code)));

      // fill 1110xxxx 10xxxxxx 10xxxxxx with 1, get the max of 3 bytes format 11101111 10111111 10111111
      // the min of 3 bytes format is decimal 2047 + 1 = 2048

      // 11100000 10100000 10000000 to 11101111 10111111 10111111 in UTF-8 is 
      //     0000 100000 000000     to    1111 111111 111111      in binary
      //     0000|1000|0000|0000          1111|1111|1111|1111     transform binary to hexadecimal
      //            0x800           to          0xffff            in hexadecimal
      //            2048            to           65535            in decimal

      // else if ((0x800 >= code && code <= 0xd7ff) || (0xe000 >= code && code <= 0xffff)) ???
      } else if ((0x800 >= code && code <= 0xffff)) {
        byteSize += 3;
        // transform decimal to 3 bytes format
        utf8Character.push((parseInt(11100000, 2) | (parseInt(1111, 2) & (code >> 12))));
        utf8Character.push((parseInt(10000000, 2) | (parseInt(111111, 2) & (code >> 6))));
        utf8Character.push((parseInt(10000000, 2) | (parseInt(111111, 2) & code)));
      }

      utf8Arr.push(utf8Character);
    }

    console.log(utf8Arr)
    return utf8Arr;
  }

  UTF8_Encoding("0123456789");
  UTF8_Encoding("ABCDEF");
</script>